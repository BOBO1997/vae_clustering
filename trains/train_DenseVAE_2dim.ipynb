{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練 (DenseVAE_2dim)\n",
    "\n",
    "## モデル\n",
    "\n",
    "### DenseVAE_2dim(\n",
    "\n",
    "-  (linear1): Linear(in_features=16384, out_features=4096, bias=True)\n",
    "-  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "-  (linear3): Linear(in_features=1024, out_features=256, bias=True)\n",
    "-  (linear4): Linear(in_features=256, out_features=64, bias=True)\n",
    "-  (linear5): Linear(in_features=64, out_features=16, bias=True)\n",
    "-  (linear_mean): Linear(in_features=16, out_features=2, bias=True)\n",
    "-  (linear_logvar): Linear(in_features=16, out_features=2, bias=True)\n",
    "-  (dec_linear6): Linear(in_features=2, out_features=16, bias=True)\n",
    "-  (dec_linear5): Linear(in_features=16, out_features=64, bias=True)\n",
    "-  (dec_linear4): Linear(in_features=64, out_features=256, bias=True)\n",
    "-  (dec_linear3): Linear(in_features=256, out_features=1024, bias=True)\n",
    "-  (dec_linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "-  (dec_linear1): Linear(in_features=4096, out_features=16384, bias=True)\n",
    ")\n",
    "\n",
    "### 潜在ベクトル: 64\n",
    "\n",
    "### 訓練\n",
    "\n",
    "- 400 epoch回したけど、もっと回しても良さそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/kwk/share/mizuho/u00257/vae_clustering/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'vae_model' from '/home/kwk/share/mizuho/u00257/vae_clustering/src/vae_model.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import vae_model\n",
    "importlib.reload(vae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae_preprocess import dataset1d\n",
    "from vae_model import DenseVAE_2dim\n",
    "from img_shows import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "data_pkls = \"../data_pkls/\"\n",
    "pkl_files = [data_pkls + \"imgs128_1.pkl\", data_pkls + \"imgs128_2.pkl\", data_pkls + \"imgs128_3.pkl\"]\n",
    "dataset = dataset1d(pkl_files)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b2dbadcbe90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset[0].to(\"cpu\").reshape(128, 128), cmap = 'gray', vmin = 0, vmax = 1, interpolation = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseVAE_2dim(\n",
      "  (linear1): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (linear3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (linear4): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (linear5): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (linear_mean): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (linear_logvar): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (dec_linear6): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (dec_linear5): Linear(in_features=16, out_features=64, bias=True)\n",
      "  (dec_linear4): Linear(in_features=64, out_features=256, bias=True)\n",
      "  (dec_linear3): Linear(in_features=256, out_features=1024, bias=True)\n",
      "  (dec_linear2): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dec_linear1): Linear(in_features=4096, out_features=16384, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"DenseVAE_2dim\"\n",
    "img_size = 128\n",
    "model = DenseVAE_2dim(img_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_save = 400\n",
    "num_epochs = 1\n",
    "train_ls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練\n",
    "\n",
    "モデルの保存はCPUへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch --- 1 train_loss: 57223330.281 (train time: 12.8 s = 0.2 min)\n",
      "epoch --- 2 train_loss: 50990464.500 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 3 train_loss: 50463057.297 (train time: 12.3 s = 0.2 min)\n",
      "epoch --- 4 train_loss: 50171152.781 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 5 train_loss: 49810972.266 (train time: 12.2 s = 0.2 min)\n",
      "epoch --- 6 train_loss: 49532186.844 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 7 train_loss: 49363367.203 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 8 train_loss: 49213167.266 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 9 train_loss: 49077445.438 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 10 train_loss: 48956497.594 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 11 train_loss: 48864321.375 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 12 train_loss: 48775565.312 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 13 train_loss: 48703948.703 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 14 train_loss: 48627196.719 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 15 train_loss: 48572783.922 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 16 train_loss: 48526275.703 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 17 train_loss: 48453237.578 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 18 train_loss: 48440995.297 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 19 train_loss: 48424948.484 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 20 train_loss: 48344162.062 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 21 train_loss: 48284770.094 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 22 train_loss: 48339377.953 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 23 train_loss: 48209083.656 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 24 train_loss: 48194892.406 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 25 train_loss: 48157807.016 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 26 train_loss: 48134394.391 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 27 train_loss: 48133052.328 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 28 train_loss: 48114283.688 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 29 train_loss: 48104417.719 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 30 train_loss: 48026900.844 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 31 train_loss: 48009463.859 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 32 train_loss: 48055025.469 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 33 train_loss: 47968494.984 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 34 train_loss: 47933482.594 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 35 train_loss: 47889611.203 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 36 train_loss: 47834078.703 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 37 train_loss: 47882408.781 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 38 train_loss: 47848398.391 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 39 train_loss: 47917165.422 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 40 train_loss: 47794387.344 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 41 train_loss: 47779149.109 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 42 train_loss: 47794497.781 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 43 train_loss: 47804359.719 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 44 train_loss: 47868110.594 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 45 train_loss: 47695626.328 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 46 train_loss: 47818452.984 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 47 train_loss: 47711554.922 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 48 train_loss: 47712411.391 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 49 train_loss: 47667085.344 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 50 train_loss: 47589930.906 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 51 train_loss: 47623774.656 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 52 train_loss: 47620924.234 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 53 train_loss: 47581130.938 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 54 train_loss: 47633506.891 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 55 train_loss: 47620178.234 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 56 train_loss: 47523941.641 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 57 train_loss: 47508880.172 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 58 train_loss: 47532530.047 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 59 train_loss: 47595823.062 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 60 train_loss: 47482010.734 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 61 train_loss: 47471700.250 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 62 train_loss: 47433242.672 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 63 train_loss: 47525822.203 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 64 train_loss: 47402765.672 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 65 train_loss: 47333158.516 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 66 train_loss: 47450764.438 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 67 train_loss: 47351617.500 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 68 train_loss: 47336524.859 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 69 train_loss: 47448282.922 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 70 train_loss: 47369402.578 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 71 train_loss: 47294193.812 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 72 train_loss: 47293831.078 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 73 train_loss: 47385840.938 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 74 train_loss: 47418624.562 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 75 train_loss: 47344266.859 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 76 train_loss: 47345557.406 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 77 train_loss: 47297374.828 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 78 train_loss: 47400309.844 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 79 train_loss: 47270070.953 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 80 train_loss: 47338286.219 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 81 train_loss: 47421589.672 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 82 train_loss: 47221201.000 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 83 train_loss: 47229901.969 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 84 train_loss: 47290465.562 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 85 train_loss: 47285206.859 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 86 train_loss: 47160743.172 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 87 train_loss: 47235092.672 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 88 train_loss: 47187743.438 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 89 train_loss: 47224765.938 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 90 train_loss: 47209323.484 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 91 train_loss: 47087329.250 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 92 train_loss: 47157490.156 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 93 train_loss: 47085168.094 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 94 train_loss: 47276319.906 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 95 train_loss: 47295574.797 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 96 train_loss: 47140443.562 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 97 train_loss: 47168737.875 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 98 train_loss: 47128085.906 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 99 train_loss: 47123222.984 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 100 train_loss: 47032345.641 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 101 train_loss: 47155164.562 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 102 train_loss: 47051722.766 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 103 train_loss: 47039218.031 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 104 train_loss: 47100280.375 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 105 train_loss: 46993385.547 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 106 train_loss: 47007730.953 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 107 train_loss: 47034357.578 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 108 train_loss: 47036486.922 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 109 train_loss: 47100131.719 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 110 train_loss: 47081001.625 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 111 train_loss: 47021004.125 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 112 train_loss: 47042564.703 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 113 train_loss: 46976404.906 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 114 train_loss: 47016226.234 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 115 train_loss: 46994034.016 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 116 train_loss: 47123874.938 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 117 train_loss: 46903515.688 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 118 train_loss: 47042029.250 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 119 train_loss: 46956271.797 (train time: 12.0 s = 0.2 min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch --- 120 train_loss: 46949485.109 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 121 train_loss: 47002443.609 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 122 train_loss: 46979559.828 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 123 train_loss: 46991751.797 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 124 train_loss: 46895905.328 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 125 train_loss: 46914461.422 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 126 train_loss: 46963485.031 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 127 train_loss: 46834918.859 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 128 train_loss: 47089916.625 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 129 train_loss: 47040594.172 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 130 train_loss: 46902646.078 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 131 train_loss: 46816124.203 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 132 train_loss: 46922895.438 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 133 train_loss: 46820302.547 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 134 train_loss: 46893570.734 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 135 train_loss: 46858691.672 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 136 train_loss: 46870033.781 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 137 train_loss: 46940033.750 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 138 train_loss: 46842168.906 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 139 train_loss: 46875019.703 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 140 train_loss: 46894626.688 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 141 train_loss: 46841438.344 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 142 train_loss: 46909415.672 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 143 train_loss: 46854336.516 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 144 train_loss: 46908703.141 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 145 train_loss: 46771825.891 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 146 train_loss: 46809230.781 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 147 train_loss: 46917386.047 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 148 train_loss: 46831075.391 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 149 train_loss: 46775766.969 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 150 train_loss: 46887713.406 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 151 train_loss: 46871395.094 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 152 train_loss: 46823284.828 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 153 train_loss: 46736157.047 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 154 train_loss: 46767891.766 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 155 train_loss: 46702227.781 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 156 train_loss: 46772193.750 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 157 train_loss: 46656243.344 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 158 train_loss: 46786783.125 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 159 train_loss: 46782135.578 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 160 train_loss: 46797608.578 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 161 train_loss: 46713143.391 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 162 train_loss: 46775714.172 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 163 train_loss: 46768310.406 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 164 train_loss: 46731057.297 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 165 train_loss: 46731301.219 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 166 train_loss: 46599186.594 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 167 train_loss: 46700354.438 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 168 train_loss: 46755246.000 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 169 train_loss: 46646400.250 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 170 train_loss: 46786567.562 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 171 train_loss: 46703926.156 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 172 train_loss: 46791841.188 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 173 train_loss: 46640028.453 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 174 train_loss: 46677931.125 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 175 train_loss: 46691937.453 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 176 train_loss: 46747058.109 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 177 train_loss: 46660445.969 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 178 train_loss: 46659486.656 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 179 train_loss: 46766296.188 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 180 train_loss: 46624179.422 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 181 train_loss: 46719980.750 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 182 train_loss: 46620452.938 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 183 train_loss: 46691679.750 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 184 train_loss: 46553658.094 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 185 train_loss: 46745080.781 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 186 train_loss: 46614388.641 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 187 train_loss: 46655514.453 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 188 train_loss: 46568439.828 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 189 train_loss: 46576506.906 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 190 train_loss: 46678943.828 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 191 train_loss: 46643030.141 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 192 train_loss: 46629004.797 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 193 train_loss: 46598087.703 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 194 train_loss: 46603339.766 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 195 train_loss: 46653862.672 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 196 train_loss: 46578039.188 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 197 train_loss: 46706046.297 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 198 train_loss: 46668881.938 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 199 train_loss: 46540783.250 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 200 train_loss: 46628090.391 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 201 train_loss: 46570749.453 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 202 train_loss: 46609120.438 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 203 train_loss: 46567751.984 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 204 train_loss: 46590870.016 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 205 train_loss: 46560124.453 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 206 train_loss: 46611037.281 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 207 train_loss: 46599651.516 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 208 train_loss: 46705340.281 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 209 train_loss: 46618159.656 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 210 train_loss: 46538724.344 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 211 train_loss: 46593455.125 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 212 train_loss: 46431276.359 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 213 train_loss: 46502939.750 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 214 train_loss: 46515877.281 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 215 train_loss: 46546710.781 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 216 train_loss: 46536161.938 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 217 train_loss: 46664280.156 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 218 train_loss: 46601932.734 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 219 train_loss: 46429009.094 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 220 train_loss: 46499919.516 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 221 train_loss: 46575700.969 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 222 train_loss: 46530966.750 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 223 train_loss: 46427835.094 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 224 train_loss: 46452261.578 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 225 train_loss: 46591054.625 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 226 train_loss: 46467008.953 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 227 train_loss: 46488302.359 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 228 train_loss: 46501659.969 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 229 train_loss: 46499248.828 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 230 train_loss: 46571821.984 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 231 train_loss: 46505161.562 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 232 train_loss: 46376645.094 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 233 train_loss: 46522098.750 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 234 train_loss: 46514797.859 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 235 train_loss: 46511039.281 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 236 train_loss: 46369842.344 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 237 train_loss: 46601802.859 (train time: 12.1 s = 0.2 min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch --- 238 train_loss: 46416857.500 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 239 train_loss: 46597800.203 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 240 train_loss: 46544351.781 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 241 train_loss: 46490739.172 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 242 train_loss: 46469622.109 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 243 train_loss: 46385660.312 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 244 train_loss: 46485559.656 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 245 train_loss: 46439427.828 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 246 train_loss: 46570481.188 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 247 train_loss: 46379871.000 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 248 train_loss: 46519829.453 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 249 train_loss: 46520057.000 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 250 train_loss: 46636684.766 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 251 train_loss: 46493917.375 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 252 train_loss: 46375224.750 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 253 train_loss: 46463866.281 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 254 train_loss: 46438944.609 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 255 train_loss: 46256391.781 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 256 train_loss: 46491337.000 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 257 train_loss: 46424173.672 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 258 train_loss: 46457340.766 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 259 train_loss: 46465347.844 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 260 train_loss: 46417343.266 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 261 train_loss: 46492020.578 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 262 train_loss: 46389769.000 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 263 train_loss: 46386906.484 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 264 train_loss: 46407938.891 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 265 train_loss: 46398734.953 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 266 train_loss: 46427997.359 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 267 train_loss: 46654724.031 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 268 train_loss: 46402176.094 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 269 train_loss: 46286930.828 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 270 train_loss: 46305920.453 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 271 train_loss: 46407462.500 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 272 train_loss: 46434772.906 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 273 train_loss: 46395952.891 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 274 train_loss: 46385400.953 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 275 train_loss: 46328952.000 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 276 train_loss: 46342045.219 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 277 train_loss: 46326394.672 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 278 train_loss: 46561625.688 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 279 train_loss: 46450577.172 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 280 train_loss: 46385530.984 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 281 train_loss: 46383517.234 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 282 train_loss: 46418336.656 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 283 train_loss: 46343860.031 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 284 train_loss: 46480357.531 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 285 train_loss: 46332996.078 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 286 train_loss: 46440390.625 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 287 train_loss: 46300069.906 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 288 train_loss: 46402073.562 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 289 train_loss: 46361214.141 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 290 train_loss: 46406218.453 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 291 train_loss: 46365090.781 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 292 train_loss: 46335942.266 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 293 train_loss: 46353910.688 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 294 train_loss: 46376161.281 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 295 train_loss: 46251255.031 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 296 train_loss: 46389338.438 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 297 train_loss: 46350245.359 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 298 train_loss: 46328913.391 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 299 train_loss: 46395628.078 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 300 train_loss: 46392721.203 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 301 train_loss: 46217216.828 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 302 train_loss: 46401331.078 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 303 train_loss: 46284530.016 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 304 train_loss: 46282199.062 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 305 train_loss: 46326661.625 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 306 train_loss: 46303419.953 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 307 train_loss: 46338393.438 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 308 train_loss: 46198587.656 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 309 train_loss: 46437075.734 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 310 train_loss: 46250841.719 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 311 train_loss: 46357202.203 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 312 train_loss: 46389038.719 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 313 train_loss: 46359716.422 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 314 train_loss: 46237190.688 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 315 train_loss: 46276463.734 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 316 train_loss: 46389105.625 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 317 train_loss: 46310315.250 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 318 train_loss: 46214539.016 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 319 train_loss: 46280948.047 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 320 train_loss: 46290054.859 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 321 train_loss: 46315461.172 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 322 train_loss: 46249867.719 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 323 train_loss: 46374440.672 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 324 train_loss: 46285362.797 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 325 train_loss: 46323298.156 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 326 train_loss: 46435274.141 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 327 train_loss: 46158322.812 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 328 train_loss: 46346325.172 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 329 train_loss: 46153616.562 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 330 train_loss: 46224647.672 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 331 train_loss: 46323591.516 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 332 train_loss: 46188014.250 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 333 train_loss: 46399902.328 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 334 train_loss: 46224106.859 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 335 train_loss: 46217855.672 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 336 train_loss: 46351311.219 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 337 train_loss: 46342206.016 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 338 train_loss: 46182408.219 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 339 train_loss: 46140951.266 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 340 train_loss: 46167357.703 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 341 train_loss: 46381773.438 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 342 train_loss: 46125055.781 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 343 train_loss: 46153097.891 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 344 train_loss: 46305028.828 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 345 train_loss: 46209663.844 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 346 train_loss: 46255199.234 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 347 train_loss: 46543945.250 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 348 train_loss: 46151321.453 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 349 train_loss: 46303891.422 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 350 train_loss: 46257256.438 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 351 train_loss: 46226982.000 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 352 train_loss: 46218552.984 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 353 train_loss: 46081880.484 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 354 train_loss: 46237341.422 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 355 train_loss: 46068118.375 (train time: 11.9 s = 0.2 min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch --- 356 train_loss: 46190964.594 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 357 train_loss: 46282224.953 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 358 train_loss: 46296185.891 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 359 train_loss: 46126597.062 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 360 train_loss: 46148046.891 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 361 train_loss: 46362435.469 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 362 train_loss: 46178516.844 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 363 train_loss: 46231538.719 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 364 train_loss: 46278509.406 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 365 train_loss: 46353895.562 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 366 train_loss: 46169900.266 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 367 train_loss: 46140722.328 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 368 train_loss: 46165599.328 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 369 train_loss: 46287940.594 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 370 train_loss: 46262646.422 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 371 train_loss: 46116470.719 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 372 train_loss: 46351363.406 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 373 train_loss: 46095863.703 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 374 train_loss: 46128926.484 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 375 train_loss: 46202768.938 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 376 train_loss: 46192566.484 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 377 train_loss: 46347965.297 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 378 train_loss: 46056774.984 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 379 train_loss: 46164588.859 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 380 train_loss: 46223563.406 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 381 train_loss: 46255838.594 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 382 train_loss: 46075470.938 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 383 train_loss: 46199118.594 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 384 train_loss: 46142663.922 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 385 train_loss: 46051351.672 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 386 train_loss: 46076591.547 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 387 train_loss: 46170063.172 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 388 train_loss: 46155610.422 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 389 train_loss: 46278896.875 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 390 train_loss: 46109363.156 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 391 train_loss: 46207378.828 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 392 train_loss: 46288143.531 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 393 train_loss: 46315935.266 (train time: 11.8 s = 0.2 min)\n",
      "epoch --- 394 train_loss: 46064199.828 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 395 train_loss: 46080971.141 (train time: 12.1 s = 0.2 min)\n",
      "epoch --- 396 train_loss: 46193609.516 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 397 train_loss: 46128746.125 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 398 train_loss: 46161264.156 (train time: 11.9 s = 0.2 min)\n",
      "epoch --- 399 train_loss: 46080087.219 (train time: 12.0 s = 0.2 min)\n",
      "epoch --- 400 train_loss: 46183041.875 (train time: 12.1 s = 0.2 min)\n"
     ]
    }
   ],
   "source": [
    "for n in range(num_save):\n",
    "    if n == 325 :\n",
    "        optimizer.defaults[\"lr\"] = 0.0005\n",
    "    if n == 375:\n",
    "        optimizer.defaults[\"lr\"] = 0.0001\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        t1 = time.time()\n",
    "        loss_sum = 0.0\n",
    "        for i, batch_imgs in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_imgs, mu, logvar = model(batch_imgs.to(device))\n",
    "            loss = loss_function(reconstructed_imgs, batch_imgs.to(device), mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        t2 = time.time()\n",
    "        train_ls.append(loss_sum)\n",
    "\n",
    "        print(\"epoch --- %d train_loss: %.3f (train time: %.1f s = %.1f min)\" %(n * num_epochs + epoch + 1, loss_sum, t2 - t1, (t2 - t1) / 60))\n",
    "    torch.save(model.state_dict(), \"../model_pths/\" + model_name + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(result, which_result, condition, save_or_not=True):\n",
    "    plt.clf()\n",
    "    plt.plot(result)\n",
    "    plt.title(condition + \"_\" + which_result)\n",
    "    plt.savefig(condition + \"_\" + which_result + \".png\") if save_or_not else plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_img_path = \"../loss_imgs/\"\n",
    "condition = \"DenseVAE_128to2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(train_ls, \"train_loss\", loss_img_path + condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = [1000,2000,3000,4000,5000,6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape () for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6a7af49cfd8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-7bba295cec08>\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(dataset, poses, model, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5677\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5679\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5680\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    689\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 690\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape () for image data"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x5400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow_dense(dataset, poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imshow(dataset, poses, model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_vecs = []\n",
    "with torch.no_grad():\n",
    "    for i, batch_imgs in enumerate(dataloader):\n",
    "        mu, logvar = model.encoder(batch_imgs)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        z = z.to(\"cpu\").numpy()\n",
    "        hidden_vecs.append(z)\n",
    "hidden_vecs = np.array(hidden_vecs)\n",
    "hidden_vecs = hidden_vecs.reshape(19154, 64)\n",
    "print(hidden_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_pkls + \"hidden_vecs_densevaedim2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hidden_vecs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(hidden_vecs.transpose()[0], hidden_vecs.transpose()[1], )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.run(['jupyter', 'nbconvert', '--to', 'python', 'vae_train_DenseVAE_2dim.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
